{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534aa7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"              # avoid DataParallel weirdness\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "import sys, torch\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor, MllamaForConditionalGeneration,\n",
    "    BitsAndBytesConfig, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json, random\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from PIL import Image\n",
    "import torch, os, re\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55749ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Use your local model copy\n",
    "LOCAL_DIR = \"/home/hice1/wyiu31/scratch/models/llama32v\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Safer default on shared GPUs (auto offload if VRAM is tight)\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    LOCAL_DIR, device_map=\"auto\", dtype=torch.float16, local_files_only=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(LOCAL_DIR, local_files_only=True)\n",
    "print(\"Model & processor loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG (edit these) ====\n",
    "BASE = \"/home/hice1/wyiu31/scratch/models/llama32v\"   # <-- base VLM, no FT\n",
    "TRAIN_JSONL = \"/home/hice1/wyiu31/scratch/llama32v_ft/data/version_11/train.jsonl\"\n",
    "VALID_JSONL = \"/home/hice1/wyiu31/scratch/llama32v_ft/data/version_11/valid.jsonl\"\n",
    "\n",
    "# where to store per-phase CSVs (one per shard)\n",
    "PARTS_DIR = \"/home/hice1/wyiu31/scratch/llama32v_ft/base_eval_parts\"\n",
    "# final consolidated CSV (after all shards done)\n",
    "FINAL_CSV = \"/home/hice1/wyiu31/scratch/llama32v_ft/base_eval_full.csv\"\n",
    "\n",
    "PROMPT = \"Identify the species; please give the common name.\"\n",
    "BURST_WINDOW_SEC   = 90\n",
    "MAX_IMGS_PER_BURST = 4\n",
    "\n",
    "# how many shards you want to split the validation into\n",
    "NUM_SHARDS = 5   # e.g. 3 phases\n",
    "SHARD_IDX  = 0   # run 0 first, then 1, then 2...\n",
    "OVERWRITE_PART = False   # False = resume this shard, True = start that shard fresh\n",
    "\n",
    "import os, random, numpy as np, torch\n",
    "os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, math, csv, time\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "TIMESTAMP_RE = re.compile(r\"__(\\d{8})_(\\d{6})__\")\n",
    "\n",
    "# modest open-world bird expansion (added if missing)\n",
    "BIRD_EXPANSION = [\n",
    "    \"Canada Goose\", \"Goose\", \"Snow Goose\",\n",
    "    \"Mallard\", \"American Black Duck\", \"Wood Duck\",\n",
    "    \"Ring-billed Gull\", \"Herring Gull\",\n",
    "    \"Great Blue Heron\", \"Great Egret\",\n",
    "    \"Mourning Dove\", \"Rock Pigeon\",\n",
    "    \"Red-tailed Hawk\", \"Red-shouldered Hawk\", \"Barred Owl\"\n",
    "]\n",
    "\n",
    "def norm_label(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"^common name[:\\s]*\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-?']\", \"\", s.lower())\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def ok_example(ex: Dict[str, Any]) -> bool:\n",
    "    return (\n",
    "        isinstance(ex.get(\"images\"), list) and ex[\"images\"] and\n",
    "        isinstance(ex.get(\"output\"), str) and ex[\"output\"].strip()\n",
    "    )\n",
    "\n",
    "def parse_dt_from_path(p: str) -> datetime | None:\n",
    "    m = TIMESTAMP_RE.search(p)\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return datetime.strptime(m.group(1)+m.group(2), \"%Y%m%d%H%M%S\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def burst_key(ex: Dict[str, Any]) -> tuple:\n",
    "    md = ex.get(\"metadata\") or {}\n",
    "    loc = (md.get(\"location\") or \"\").strip()\n",
    "\n",
    "    dt = None\n",
    "    imgs = ex.get(\"images\") or []\n",
    "    if imgs:\n",
    "        dt = parse_dt_from_path(imgs[0])\n",
    "    if dt is None:\n",
    "        date_str = (md.get(\"date\") or \"\").strip()\n",
    "        time_str = (md.get(\"time\") or \"00:00:00\").strip()\n",
    "        try:\n",
    "            if date_str and time_str:\n",
    "                dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            elif date_str:\n",
    "                dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            dt = None\n",
    "\n",
    "    if dt is None:\n",
    "        return (loc, \"unknown\", id(ex))\n",
    "\n",
    "    bucket = int(dt.timestamp() // BURST_WINDOW_SEC)\n",
    "    day = dt.strftime(\"%Y-%m-%d\")\n",
    "    return (loc, day, bucket)\n",
    "\n",
    "def make_burst_id(key_tuple: tuple) -> str:\n",
    "    loc, day, bucket = key_tuple\n",
    "    return f\"{loc}|{day}|{bucket}\"\n",
    "\n",
    "def merge_to_bursts(examples):\n",
    "    groups = defaultdict(list)\n",
    "    for ex in examples:\n",
    "        groups[burst_key(ex)].append(ex)\n",
    "\n",
    "    merged = []\n",
    "    for key, items in groups.items():\n",
    "        seen = set(); imgs = []\n",
    "        for ex in items:\n",
    "            for p in (ex.get(\"images\") or []):\n",
    "                if isinstance(p, str) and p not in seen:\n",
    "                    imgs.append(p); seen.add(p)\n",
    "        if not imgs:\n",
    "            continue\n",
    "\n",
    "        label = None\n",
    "        for ex in items:\n",
    "            out = ex.get(\"output\")\n",
    "            if isinstance(out, str) and out.strip():\n",
    "                label = out.strip()\n",
    "                break\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        md0 = items[0].get(\"metadata\") or {}\n",
    "        merged.append({\n",
    "            \"burst_id\": make_burst_id(key),\n",
    "            \"images\": imgs[:MAX_IMGS_PER_BURST],\n",
    "            \"output\": label,\n",
    "            \"metadata\": {**md0, \"burst_size\": len(imgs), \"burst_group_count\": len(items)},\n",
    "        })\n",
    "    return merged\n",
    "\n",
    "def load_candidates(*jsonls) -> List[str]:\n",
    "    cnt = Counter()\n",
    "    raw_map = {}\n",
    "    for p in jsonls:\n",
    "        if not os.path.exists(p): \n",
    "            continue\n",
    "        with open(p) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    ex = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                out = ex.get(\"output\", \"\")\n",
    "                if not isinstance(out, str) or not out.strip():\n",
    "                    continue\n",
    "                raw = out.strip()\n",
    "                nn = norm_label(raw)\n",
    "                if not nn:\n",
    "                    continue\n",
    "                cnt[nn] += 1\n",
    "                raw_map.setdefault(nn, raw)\n",
    "\n",
    "    labels = [raw_map[nn] for nn,_ in cnt.items()]\n",
    "\n",
    "    # add common birds if missing\n",
    "    have_norm = {norm_label(x) for x in labels}\n",
    "    for bird in BIRD_EXPANSION:\n",
    "        if norm_label(bird) not in have_norm:\n",
    "            labels.append(bird)\n",
    "\n",
    "    return sorted(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c32ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- load valid jsonl and build bursts ----\n",
    "raw = []\n",
    "with open(VALID_JSONL) as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            ex = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if ok_example(ex):\n",
    "            raw.append(ex)\n",
    "\n",
    "print(f\"Loaded raw validation examples: {len(raw):,}\")\n",
    "bursts_all = merge_to_bursts(raw)\n",
    "print(f\"After burst merge (window={BURST_WINDOW_SEC}s): {len(bursts_all):,} bursts\")\n",
    "\n",
    "candidates = load_candidates(TRAIN_JSONL, VALID_JSONL)\n",
    "print(f\"Candidates: {len(candidates)} (sample: {candidates[:8]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model_and_processor(base_path: str):\n",
    "    processor = AutoProcessor.from_pretrained(base_path, local_files_only=True)\n",
    "    tok = processor.tokenizer\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    print(\"Loading BASE model only (no fine-tuned adapter)â€¦\")\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        base_path,\n",
    "        quantization_config=bnb,\n",
    "        device_map={\"\": 0} if device == \"cuda\" else None,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    try:\n",
    "        model.config._attn_implementation = \"sdpa\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "    return model, processor\n",
    "\n",
    "model, processor = load_base_model_and_processor(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d58da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enc(processor, imgs: List[Image.Image]):\n",
    "    msgs = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [*({\"type\":\"image\",\"image\":im} for im in imgs),\n",
    "                    {\"type\":\"text\",\"text\": PROMPT}],\n",
    "    }]\n",
    "    txt = processor.apply_chat_template(\n",
    "        msgs, add_generation_prompt=True, tokenize=True,\n",
    "        return_tensors=\"pt\", return_dict=True, padding=False, truncation=True\n",
    "    )\n",
    "    vis = processor(images=imgs, return_tensors=\"pt\")\n",
    "    enc = {**dict(txt), **dict(vis)}\n",
    "    return {k: (v.to(device) if torch.is_tensor(v) else v) for k,v in enc.items()}\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_candidate(model, processor, enc, candidate_text: str) -> float:\n",
    "    base_input_ids = enc[\"input_ids\"]\n",
    "    cand_ids = processor.tokenizer(candidate_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(device)\n",
    "    input_ids = torch.cat([base_input_ids, cand_ids], dim=-1)\n",
    "    attn = torch.cat([enc.get(\"attention_mask\", torch.ones_like(base_input_ids)), torch.ones_like(cand_ids)], dim=-1)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attn,\n",
    "        pixel_values=enc.get(\"pixel_values\"),\n",
    "        aspect_ratio_mask=enc.get(\"aspect_ratio_mask\"),\n",
    "        aspect_ratio_ids=enc.get(\"aspect_ratio_ids\"),\n",
    "        use_cache=False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    logits = out.logits[0]\n",
    "    start = base_input_ids.shape[1] - 1\n",
    "    end   = input_ids.shape[1] - 1\n",
    "    if end <= start:\n",
    "        return -1e9\n",
    "    logp = torch.log_softmax(logits[start:end, :], dim=-1)\n",
    "    target = input_ids[0, start+1:end+1].unsqueeze(-1)\n",
    "    tok_logp = logp.gather(-1, target).squeeze(-1)\n",
    "    return float(tok_logp.mean().item()) if tok_logp.numel() else -1e9\n",
    "\n",
    "@torch.inference_mode()\n",
    "def infer_burst_scoring(model, processor, image_paths: List[str], candidates: List[str]) -> str:\n",
    "    imgs = []\n",
    "    for p in image_paths:\n",
    "        try:\n",
    "            imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not imgs:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    enc = build_enc(processor, imgs)\n",
    "    best_s, best_c = -1e9, \"Unknown\"\n",
    "    for c in candidates:\n",
    "        s = score_candidate(model, processor, enc, c)\n",
    "        if s > best_s:\n",
    "            best_s, best_c = s, c\n",
    "    return best_c\n",
    "\n",
    "def take_shard(items, num_shards: int, shard_idx: int):\n",
    "    if num_shards <= 1:\n",
    "        return list(items)\n",
    "    return [items[i] for i in range(len(items)) if (i % num_shards) == shard_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_dir = Path(PARTS_DIR)\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "part_path = parts_dir / f\"base_part_{SHARD_IDX:02d}.csv\"\n",
    "header = [\"burst_id\",\"gold\",\"pred\",\"correct\",\"images\",\"burst_size\"]\n",
    "\n",
    "# get subset for this shard\n",
    "bursts_shard = take_shard(bursts_all, NUM_SHARDS, SHARD_IDX)\n",
    "print(f\"Shard {SHARD_IDX}/{NUM_SHARDS}: {len(bursts_shard)} bursts\")\n",
    "\n",
    "# read existing rows if resuming\n",
    "existing_ids = set()\n",
    "if part_path.exists() and not OVERWRITE_PART:\n",
    "    with part_path.open() as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            existing_ids.add(row[\"burst_id\"])\n",
    "\n",
    "mode = \"w\" if (OVERWRITE_PART or not part_path.exists()) else \"a\"\n",
    "f = part_path.open(mode, newline=\"\")\n",
    "w = csv.DictWriter(f, fieldnames=header)\n",
    "if mode == \"w\":\n",
    "    w.writeheader()\n",
    "\n",
    "tot = ok = 0\n",
    "per_class = defaultdict(lambda: {\"n\":0, \"ok\":0})\n",
    "\n",
    "try:\n",
    "    for ex in tqdm(bursts_shard, desc=f\"Validate BASE (shard {SHARD_IDX})\"):\n",
    "        bid = ex[\"burst_id\"]\n",
    "        if bid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        gold_raw = ex[\"output\"]; gold = norm_label(gold_raw)\n",
    "        pred_raw = infer_burst_scoring(model, processor, ex[\"images\"], candidates)\n",
    "        pred = norm_label(pred_raw)\n",
    "\n",
    "        hit = int(pred == gold)\n",
    "        tot += 1; ok += hit\n",
    "        per_class[gold][\"n\"] += 1\n",
    "        per_class[gold][\"ok\"] += hit\n",
    "\n",
    "        row = {\n",
    "            \"burst_id\": bid,\n",
    "            \"gold\": gold_raw,\n",
    "            \"pred\": pred_raw,\n",
    "            \"correct\": hit,\n",
    "            \"images\": \"|\".join(ex[\"images\"]),\n",
    "            \"burst_size\": ex.get(\"metadata\",{}).get(\"burst_size\", len(ex[\"images\"])),\n",
    "        }\n",
    "        w.writerow(row)\n",
    "        f.flush()   # important for long runs\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "acc = ok / max(1, tot)\n",
    "print(f\"\\n[BASE Shard {SHARD_IDX}] Burst exact-match accuracy: {acc:.4f} ({ok}/{tot})\")\n",
    "top = sorted(per_class.items(), key=lambda kv: -kv[1][\"n\"])[:15]\n",
    "print(\"\\n[BASE Shard] Per-class (top 15):\")\n",
    "for cls, st in top:\n",
    "    acc_c = (st[\"ok\"] / st[\"n\"]) if st[\"n\"] else 0.0\n",
    "    print(f\"{cls:<30s}  {st['ok']:>4d}/{st['n']:<4d}  {acc_c:6.3f}\")\n",
    "\n",
    "print(f\"\\nShard written to: {part_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8edd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shard index each time (from 0 to 4)\n",
    "SHARD_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_dir = Path(PARTS_DIR)\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "part_path = parts_dir / f\"base_part_{SHARD_IDX:02d}.csv\"\n",
    "header = [\"burst_id\",\"gold\",\"pred\",\"correct\",\"images\",\"burst_size\"]\n",
    "\n",
    "# get subset for this shard\n",
    "bursts_shard = take_shard(bursts_all, NUM_SHARDS, SHARD_IDX)\n",
    "print(f\"Shard {SHARD_IDX}/{NUM_SHARDS}: {len(bursts_shard)} bursts\")\n",
    "\n",
    "# read existing rows if resuming\n",
    "existing_ids = set()\n",
    "if part_path.exists() and not OVERWRITE_PART:\n",
    "    with part_path.open() as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            existing_ids.add(row[\"burst_id\"])\n",
    "\n",
    "mode = \"w\" if (OVERWRITE_PART or not part_path.exists()) else \"a\"\n",
    "f = part_path.open(mode, newline=\"\")\n",
    "w = csv.DictWriter(f, fieldnames=header)\n",
    "if mode == \"w\":\n",
    "    w.writeheader()\n",
    "\n",
    "tot = ok = 0\n",
    "per_class = defaultdict(lambda: {\"n\":0, \"ok\":0})\n",
    "\n",
    "try:\n",
    "    for ex in tqdm(bursts_shard, desc=f\"Validate BASE (shard {SHARD_IDX})\"):\n",
    "        bid = ex[\"burst_id\"]\n",
    "        if bid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        gold_raw = ex[\"output\"]; gold = norm_label(gold_raw)\n",
    "        pred_raw = infer_burst_scoring(model, processor, ex[\"images\"], candidates)\n",
    "        pred = norm_label(pred_raw)\n",
    "\n",
    "        hit = int(pred == gold)\n",
    "        tot += 1; ok += hit\n",
    "        per_class[gold][\"n\"] += 1\n",
    "        per_class[gold][\"ok\"] += hit\n",
    "\n",
    "        row = {\n",
    "            \"burst_id\": bid,\n",
    "            \"gold\": gold_raw,\n",
    "            \"pred\": pred_raw,\n",
    "            \"correct\": hit,\n",
    "            \"images\": \"|\".join(ex[\"images\"]),\n",
    "            \"burst_size\": ex.get(\"metadata\",{}).get(\"burst_size\", len(ex[\"images\"])),\n",
    "        }\n",
    "        w.writerow(row)\n",
    "        f.flush()   # important for long runs\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "acc = ok / max(1, tot)\n",
    "print(f\"\\n[BASE Shard {SHARD_IDX}] Burst exact-match accuracy: {acc:.4f} ({ok}/{tot})\")\n",
    "top = sorted(per_class.items(), key=lambda kv: -kv[1][\"n\"])[:15]\n",
    "print(\"\\n[BASE Shard] Per-class (top 15):\")\n",
    "for cls, st in top:\n",
    "    acc_c = (st[\"ok\"] / st[\"n\"]) if st[\"n\"] else 0.0\n",
    "    print(f\"{cls:<30s}  {st['ok']:>4d}/{st['n']:<4d}  {acc_c:6.3f}\")\n",
    "\n",
    "print(f\"\\nShard written to: {part_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c93d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shard index each time (from 0 to 4)\n",
    "SHARD_IDX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_dir = Path(PARTS_DIR)\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "part_path = parts_dir / f\"base_part_{SHARD_IDX:02d}.csv\"\n",
    "header = [\"burst_id\",\"gold\",\"pred\",\"correct\",\"images\",\"burst_size\"]\n",
    "\n",
    "# get subset for this shard\n",
    "bursts_shard = take_shard(bursts_all, NUM_SHARDS, SHARD_IDX)\n",
    "print(f\"Shard {SHARD_IDX}/{NUM_SHARDS}: {len(bursts_shard)} bursts\")\n",
    "\n",
    "# read existing rows if resuming\n",
    "existing_ids = set()\n",
    "if part_path.exists() and not OVERWRITE_PART:\n",
    "    with part_path.open() as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            existing_ids.add(row[\"burst_id\"])\n",
    "\n",
    "mode = \"w\" if (OVERWRITE_PART or not part_path.exists()) else \"a\"\n",
    "f = part_path.open(mode, newline=\"\")\n",
    "w = csv.DictWriter(f, fieldnames=header)\n",
    "if mode == \"w\":\n",
    "    w.writeheader()\n",
    "\n",
    "tot = ok = 0\n",
    "per_class = defaultdict(lambda: {\"n\":0, \"ok\":0})\n",
    "\n",
    "try:\n",
    "    for ex in tqdm(bursts_shard, desc=f\"Validate BASE (shard {SHARD_IDX})\"):\n",
    "        bid = ex[\"burst_id\"]\n",
    "        if bid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        gold_raw = ex[\"output\"]; gold = norm_label(gold_raw)\n",
    "        pred_raw = infer_burst_scoring(model, processor, ex[\"images\"], candidates)\n",
    "        pred = norm_label(pred_raw)\n",
    "\n",
    "        hit = int(pred == gold)\n",
    "        tot += 1; ok += hit\n",
    "        per_class[gold][\"n\"] += 1\n",
    "        per_class[gold][\"ok\"] += hit\n",
    "\n",
    "        row = {\n",
    "            \"burst_id\": bid,\n",
    "            \"gold\": gold_raw,\n",
    "            \"pred\": pred_raw,\n",
    "            \"correct\": hit,\n",
    "            \"images\": \"|\".join(ex[\"images\"]),\n",
    "            \"burst_size\": ex.get(\"metadata\",{}).get(\"burst_size\", len(ex[\"images\"])),\n",
    "        }\n",
    "        w.writerow(row)\n",
    "        f.flush()   # important for long runs\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "acc = ok / max(1, tot)\n",
    "print(f\"\\n[BASE Shard {SHARD_IDX}] Burst exact-match accuracy: {acc:.4f} ({ok}/{tot})\")\n",
    "top = sorted(per_class.items(), key=lambda kv: -kv[1][\"n\"])[:15]\n",
    "print(\"\\n[BASE Shard] Per-class (top 15):\")\n",
    "for cls, st in top:\n",
    "    acc_c = (st[\"ok\"] / st[\"n\"]) if st[\"n\"] else 0.0\n",
    "    print(f\"{cls:<30s}  {st['ok']:>4d}/{st['n']:<4d}  {acc_c:6.3f}\")\n",
    "\n",
    "print(f\"\\nShard written to: {part_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shard index each time (from 0 to 4)\n",
    "SHARD_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_dir = Path(PARTS_DIR)\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "part_path = parts_dir / f\"base_part_{SHARD_IDX:02d}.csv\"\n",
    "header = [\"burst_id\",\"gold\",\"pred\",\"correct\",\"images\",\"burst_size\"]\n",
    "\n",
    "# get subset for this shard\n",
    "bursts_shard = take_shard(bursts_all, NUM_SHARDS, SHARD_IDX)\n",
    "print(f\"Shard {SHARD_IDX}/{NUM_SHARDS}: {len(bursts_shard)} bursts\")\n",
    "\n",
    "# read existing rows if resuming\n",
    "existing_ids = set()\n",
    "if part_path.exists() and not OVERWRITE_PART:\n",
    "    with part_path.open() as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            existing_ids.add(row[\"burst_id\"])\n",
    "\n",
    "mode = \"w\" if (OVERWRITE_PART or not part_path.exists()) else \"a\"\n",
    "f = part_path.open(mode, newline=\"\")\n",
    "w = csv.DictWriter(f, fieldnames=header)\n",
    "if mode == \"w\":\n",
    "    w.writeheader()\n",
    "\n",
    "tot = ok = 0\n",
    "per_class = defaultdict(lambda: {\"n\":0, \"ok\":0})\n",
    "\n",
    "try:\n",
    "    for ex in tqdm(bursts_shard, desc=f\"Validate BASE (shard {SHARD_IDX})\"):\n",
    "        bid = ex[\"burst_id\"]\n",
    "        if bid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        gold_raw = ex[\"output\"]; gold = norm_label(gold_raw)\n",
    "        pred_raw = infer_burst_scoring(model, processor, ex[\"images\"], candidates)\n",
    "        pred = norm_label(pred_raw)\n",
    "\n",
    "        hit = int(pred == gold)\n",
    "        tot += 1; ok += hit\n",
    "        per_class[gold][\"n\"] += 1\n",
    "        per_class[gold][\"ok\"] += hit\n",
    "\n",
    "        row = {\n",
    "            \"burst_id\": bid,\n",
    "            \"gold\": gold_raw,\n",
    "            \"pred\": pred_raw,\n",
    "            \"correct\": hit,\n",
    "            \"images\": \"|\".join(ex[\"images\"]),\n",
    "            \"burst_size\": ex.get(\"metadata\",{}).get(\"burst_size\", len(ex[\"images\"])),\n",
    "        }\n",
    "        w.writerow(row)\n",
    "        f.flush()   # important for long runs\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "acc = ok / max(1, tot)\n",
    "print(f\"\\n[BASE Shard {SHARD_IDX}] Burst exact-match accuracy: {acc:.4f} ({ok}/{tot})\")\n",
    "top = sorted(per_class.items(), key=lambda kv: -kv[1][\"n\"])[:15]\n",
    "print(\"\\n[BASE Shard] Per-class (top 15):\")\n",
    "for cls, st in top:\n",
    "    acc_c = (st[\"ok\"] / st[\"n\"]) if st[\"n\"] else 0.0\n",
    "    print(f\"{cls:<30s}  {st['ok']:>4d}/{st['n']:<4d}  {acc_c:6.3f}\")\n",
    "\n",
    "print(f\"\\nShard written to: {part_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d04fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shard index each time (from 0 to 4)\n",
    "SHARD_IDX = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5472ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_dir = Path(PARTS_DIR)\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "part_path = parts_dir / f\"base_part_{SHARD_IDX:02d}.csv\"\n",
    "header = [\"burst_id\",\"gold\",\"pred\",\"correct\",\"images\",\"burst_size\"]\n",
    "\n",
    "# get subset for this shard\n",
    "bursts_shard = take_shard(bursts_all, NUM_SHARDS, SHARD_IDX)\n",
    "print(f\"Shard {SHARD_IDX}/{NUM_SHARDS}: {len(bursts_shard)} bursts\")\n",
    "\n",
    "# read existing rows if resuming\n",
    "existing_ids = set()\n",
    "if part_path.exists() and not OVERWRITE_PART:\n",
    "    with part_path.open() as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            existing_ids.add(row[\"burst_id\"])\n",
    "\n",
    "mode = \"w\" if (OVERWRITE_PART or not part_path.exists()) else \"a\"\n",
    "f = part_path.open(mode, newline=\"\")\n",
    "w = csv.DictWriter(f, fieldnames=header)\n",
    "if mode == \"w\":\n",
    "    w.writeheader()\n",
    "\n",
    "tot = ok = 0\n",
    "per_class = defaultdict(lambda: {\"n\":0, \"ok\":0})\n",
    "\n",
    "try:\n",
    "    for ex in tqdm(bursts_shard, desc=f\"Validate BASE (shard {SHARD_IDX})\"):\n",
    "        bid = ex[\"burst_id\"]\n",
    "        if bid in existing_ids:\n",
    "            continue\n",
    "\n",
    "        gold_raw = ex[\"output\"]; gold = norm_label(gold_raw)\n",
    "        pred_raw = infer_burst_scoring(model, processor, ex[\"images\"], candidates)\n",
    "        pred = norm_label(pred_raw)\n",
    "\n",
    "        hit = int(pred == gold)\n",
    "        tot += 1; ok += hit\n",
    "        per_class[gold][\"n\"] += 1\n",
    "        per_class[gold][\"ok\"] += hit\n",
    "\n",
    "        row = {\n",
    "            \"burst_id\": bid,\n",
    "            \"gold\": gold_raw,\n",
    "            \"pred\": pred_raw,\n",
    "            \"correct\": hit,\n",
    "            \"images\": \"|\".join(ex[\"images\"]),\n",
    "            \"burst_size\": ex.get(\"metadata\",{}).get(\"burst_size\", len(ex[\"images\"])),\n",
    "        }\n",
    "        w.writerow(row)\n",
    "        f.flush()   # important for long runs\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "acc = ok / max(1, tot)\n",
    "print(f\"\\n[BASE Shard {SHARD_IDX}] Burst exact-match accuracy: {acc:.4f} ({ok}/{tot})\")\n",
    "top = sorted(per_class.items(), key=lambda kv: -kv[1][\"n\"])[:15]\n",
    "print(\"\\n[BASE Shard] Per-class (top 15):\")\n",
    "for cls, st in top:\n",
    "    acc_c = (st[\"ok\"] / st[\"n\"]) if st[\"n\"] else 0.0\n",
    "    print(f\"{cls:<30s}  {st['ok']:>4d}/{st['n']:<4d}  {acc_c:6.3f}\")\n",
    "\n",
    "print(f\"\\nShard written to: {part_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf1935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama32v (scratch)",
   "language": "python",
   "name": "llama32v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
